{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZzH89YK62BUK/hi+pbdBz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti121-2025s2/blob/main/L4/Grad_Cam_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grad-CAM\n",
        "\n",
        "This lab exercise show how you can use Grad-CAM to visualize what features are important in influencing the model prediction.\n",
        "\n",
        "We will be using [pytorch-gradcam](https://github.com/jacobgil/pytorch-grad-cam) package for this exercise"
      ],
      "metadata": {
        "id": "ACHFKXnE52rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opoNZRY6jJHZ"
      },
      "outputs": [],
      "source": [
        "%pip install grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import (\n",
        "    show_cam_on_image, deprocess_image, preprocess_image\n",
        ")\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, ClassifierOutputReST"
      ],
      "metadata": {
        "id": "NWWXjaV5j89i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will load the image using cv2 library. We scale the image to be between 0 and 1.0 and further process the image to have the means and std deviations required for resnet as we will be using resnet for our image classification task.\n",
        "\n",
        "Different pretrained network will have different ways to process the image (rescaling, resizing, etc).\n",
        "\n",
        "You can find different transformation that is required by looking at the `transform()` method bundled with the weights.\n",
        "\n",
        "```python\n",
        "\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "aOkoZZLJ7APr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('cockatoo.jpeg')\n",
        "\n",
        "rgb_img = cv2.imread('cockatoo.jpeg', 1)[:, :, ::-1]\n",
        "rgb_img = np.float32(rgb_img) / 255\n",
        "input_tensor = preprocess_image(rgb_img,\n",
        "                                    mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225]).to(\"cpu\")\n"
      ],
      "metadata": {
        "id": "Oy7JDZa0kBfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the pretrained network Resnet50 for predicting the image.  \n",
        "\n",
        "You will need to choose the target layer you want to compute the visualization for.\n",
        "Usually this will be the last convolutional layer in the model.\n",
        "Some common choices can be:\n",
        "- Resnet18 and 50: model.layer4\n",
        "- VGG, densenet161: model.features[-1]\n",
        "- mnasnet1_0: model.layers[-1]\n",
        "\n",
        "You can print the model to help chose the layer\n",
        "You can pass a list with several target layers,\n",
        "in that case the CAMs will be computed per layer and then aggregated."
      ],
      "metadata": {
        "id": "DnjD1J8La-g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the output directory to write the visualization to\n",
        "output_dir = \"output\"\n",
        "output_file = \"GradCam_cam.jpg\"\n",
        "\n",
        "# use CPU for computation\n",
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "AWhWOWNJcAwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(pretrained=True).to(device).eval()\n",
        "target_layers = [model.layer4]\n"
      ],
      "metadata": {
        "id": "EZsOcQkmolKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# targets = None    # If targets is None, the highest scoring category (for every member in the batch) will be used.\n",
        "targets = [ClassifierOutputTarget(89)]   # Take the gradient of the score for class 281 w.r.t. the convolutional activations.”\n",
        "# targets = [ClassifierOutputReST(89)] # Highlight regions that specifically distinguish one class from the rest. This often produces sharper, more discriminative heatmaps,\n",
        "\n",
        "\n",
        "cam_algorithm = GradCAM\n",
        "with cam_algorithm(model=model,\n",
        "                    target_layers=target_layers) as cam:\n",
        "\n",
        "    # cam.batch_size = 32\n",
        "    grayscale_cam = cam(input_tensor=input_tensor,\n",
        "                        targets=targets,\n",
        "                        aug_smooth=True,  # Apply test time augmentation to smooth the CAM\n",
        "                        eigen_smooth=True) # Reduce noise by taking the first principle component\n",
        "\n",
        "    # heat map\n",
        "    grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "    # overlays the Grad-CAM heatmap (grayscale intensity) on top of the original RGB image — so you can visually see where the model is focusing.\n",
        "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    # OpenCV internally represents images in BGR order by default.\n",
        "    cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "cam_output_path = os.path.join(output_dir, output_file)\n",
        "\n",
        "cv2.imwrite(cam_output_path, cam_image)\n"
      ],
      "metadata": {
        "id": "yDgFSSACo9-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ydAcR8Ndifni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's display the resultant viualizatio."
      ],
      "metadata": {
        "id": "bGBkkaaQigkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread(cam_output_path)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "utJTlvEuAyag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}