{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwhCQqzRH2WhdzoBSl6qif",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti121-2025s2/blob/main/L4/Grad_Cam_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grad-CAM\n",
        "\n",
        "This lab exercise show how you can use Grad-CAM to visualize what features are important in influencing the model prediction.\n",
        "\n",
        "We will be using [pytorch-gradcam](https://github.com/jacobgil/pytorch-grad-cam) package for this exercise"
      ],
      "metadata": {
        "id": "ACHFKXnE52rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights"
      ],
      "metadata": {
        "id": "OsycX2Na85ON"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Weight Transforms\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "\n"
      ],
      "metadata": {
        "id": "PzmDQx0T88sI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1LQu2pz-Ldl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights.transforms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1TSjF-v9Iy4",
        "outputId": "eb0a40c5-0470-40a9-fcda-4cb9cbf51da1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[232]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BILINEAR\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opoNZRY6jJHZ"
      },
      "outputs": [],
      "source": [
        "%pip install grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
        "from pytorch_grad_cam.utils.image import (\n",
        "    show_cam_on_image, deprocess_image, preprocess_image\n",
        ")\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, ClassifierOutputReST"
      ],
      "metadata": {
        "id": "NWWXjaV5j89i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will load the image using cv2 library. We scale the image to be between 0 and 1.0 and further process the image to have the means and std deviations required for resnet as we will be using resnet for our image classification task."
      ],
      "metadata": {
        "id": "aOkoZZLJ7APr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('cockatoo.jpeg')\n",
        "\n",
        "rgb_img = cv2.imread('cockatoo.jpeg', 1)[:, :, ::-1]\n",
        "rgb_img = np.float32(rgb_img) / 255\n",
        "input_tensor = preprocess_image(rgb_img,\n",
        "                                    mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225]).to(\"cpu\")\n"
      ],
      "metadata": {
        "id": "Oy7JDZa0kBfn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor2 = preprocess(Image.fromarray((rgb_img * 255).astype(np.uint8)))"
      ],
      "metadata": {
        "id": "oTH5yCVT-NID"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor2[0, 0, :].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph_lFyAV-v8U",
        "outputId": "4d2e0527-8ba6-49d0-f2fc-6b9caa66fec0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.3533)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor[0,0,:].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcsTU34U9pza",
        "outputId": "e8ae2bda-2781-498e-9063-351d69346bda"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.7483)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet50(pretrained=True).to(torch.device(\"cpu\")).eval()\n",
        "target_layers = [model.layer4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZsOcQkmolKP",
        "outputId": "1e92e0d9-491f-4738-e21e-5d513484fa89"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 90.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets = None\n",
        "\n",
        "    # Using the with statement ensures the context is freed, and you can\n",
        "    # recreate different CAM objects in a loop.\n",
        "cam_algorithm = GradCAM\n",
        "with cam_algorithm(model=model,\n",
        "                    target_layers=target_layers) as cam:\n",
        "\n",
        "    # AblationCAM and ScoreCAM have batched implementations.\n",
        "    # You can override the internal batch size for faster computation.\n",
        "    cam.batch_size = 32\n",
        "    grayscale_cam = cam(input_tensor=input_tensor,\n",
        "                        targets=targets,\n",
        "                        aug_smooth=True,\n",
        "                        eigen_smooth=True)\n",
        "\n",
        "    grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "    cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "gb_model = GuidedBackpropReLUModel(model=model, device=\"cpu\")\n",
        "gb = gb_model(input_tensor, target_category=None)\n",
        "\n",
        "cam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam])\n",
        "cam_gb = deprocess_image(cam_mask * gb)\n",
        "gb = deprocess_image(gb)\n",
        "\n",
        "import os\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "\n",
        "cam_output_path = os.path.join(\"output\", 'GradCam_cam.jpg')\n",
        "gb_output_path = os.path.join(\"output\", 'GradCam_gb.jpg')\n",
        "cam_gb_output_path = os.path.join(\"output\", 'GradCam_cam_gb.jpg')\n",
        "\n",
        "cv2.imwrite(cam_output_path, cam_image)\n",
        "cv2.imwrite(gb_output_path, gb)\n",
        "cv2.imwrite(cam_gb_output_path, cam_gb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDgFSSACo9-m",
        "outputId": "cfa9ba9c-92a0-47c2-c044-22328c12056e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "model = resnet50(pretrained=True)\n",
        "target_layers = [model.layer4[-1]]\n",
        "\n",
        "# input_tensor = # Create an input tensor image for your model..\n",
        "\n",
        "# Note: input_tensor can be a batch tensor with several images!\n",
        "\n",
        "# Construct the CAM object once, and then re-use it on many images:\n",
        "cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "# You can also use it within a with statement, to make sure it is freed,\n",
        "# In case you need to re-create it inside an outer loop:\n",
        "# with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam:\n",
        "#   ...\n",
        "\n",
        "# We have to specify the target we want to generate\n",
        "# the Class Activation Maps for.\n",
        "# If targets is None, the highest scoring category\n",
        "# will be used for every image in the batch.\n",
        "# Here we use ClassifierOutputTarget, but you can define your own custom targets\n",
        "# That are, for example, combinations of categories, or specific outputs in a non standard model.\n",
        "\n",
        "targets = [ClassifierOutputTarget(281)]\n",
        "\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# In this example grayscale_cam has only one image in the batch:\n",
        "grayscale_cam = grayscale_cam[0, :]\n",
        "visualization = show_cam_on_image(tensor/255.0, grayscale_cam, use_rgb=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "gkN_MhbqjaWi",
        "outputId": "f59cdeb7-2996-4bbf-c9b1-0b34e03ea8af"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "max() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim = False)\n      didn't match because some of the keywords were incorrect: out, axis\n * (name dim, bool keepdim = False)\n      didn't match because some of the keywords were incorrect: out, axis\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4001326787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# In this example grayscale_cam has only one image in the batch:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mgrayscale_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrayscale_cam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mvisualization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_cam_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale_cam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_rgb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pytorch_grad_cam/utils/image.py\u001b[0m in \u001b[0;36mshow_cam_on_image\u001b[0;34m(img, mask, use_rgb, colormap, image_weight)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         raise Exception(\n\u001b[1;32m     57\u001b[0m             \"The input image should np.float32 in the range [0, 1]\")\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2897\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m     \"\"\"\n\u001b[0;32m-> 2899\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2900\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim = False)\n      didn't match because some of the keywords were incorrect: out, axis\n * (name dim, bool keepdim = False)\n      didn't match because some of the keywords were incorrect: out, axis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1hqcd-bkVIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}