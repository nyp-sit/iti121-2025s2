{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024s2/blob/main/session-3/yolo8_custom_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9fM_zdIhpGt"
      },
      "source": [
        "# Object Detection using YOLO\n",
        "\n",
        "Welcome to this week's hands-on lab. In this lab, we are going to learn how to train a balloon detector!\n",
        "\n",
        "At the end of this exercise, you will be able to:\n",
        "\n",
        "- create an object detection dataset in YOLO format\n",
        "- fine-tune a YOLOv8 pretrained model with the custom dataset\n",
        "- monitor the training progress and evaluation metrics\n",
        "- deploy the trained model for object detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an object detection dataset\n",
        "\n",
        "We will use a sample balloon dataset to illustrate the process of annotation and packaging the dataset into different format for object detection (e.g. YOLO, Pascal VOC, COCO, etc).\n",
        "\n",
        "To annotate, there are many different tools available, such as the very basic [LabelImg](https://github.com/HumanSignal/labelImg) , or the more feature-packed tool such as [Label Studio](https://labelstud.io/), or online service such as [Roboflow](https://roboflow.com/).\n",
        "\n",
        "### Raw Image Dataset\n",
        "\n",
        "You can download the balloon images (without annotations) from this link:\n",
        "\n",
        "https://github.com/nyp-sit/iti107-2024S2/raw/refs/heads/main/data/balloon_raw_dataset.zip\n",
        "\n",
        "Unzip the file to a local folder.\n",
        "\n",
        "There are total of 74 images. You should divide the images into both training and validation set (e.g. 80%-20%, i.e. 59 images for train, and 15 for test).\n"
      ],
      "metadata": {
        "id": "vWSUqz7gPOF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 1: Label Studio\n",
        "\n",
        "You can follow the [steps](https://labelstud.io/guide/quick_start) here to setup Label Studio on your PC. It is recommended to setup a conda environment before you install the Label Studio.  \n",
        "\n",
        "Here are the steps that need to be done:\n",
        "1. Create a new Project\n",
        "2. Import the images into Label Studio\n",
        "3. Set up the Labelling UI tempalte (choose Object Detection with Bounding Box template)\n",
        "4. Export the dataset in YOLO format.\n",
        "\n",
        "The exported dataset will have the following folder structure:\n",
        "```\n",
        "<root folder>\n",
        "classes.txt    --> contains the labels, with each class label on a new line\n",
        "--images --> contains the images\n",
        "--labels --> contains the annotations (i.e. bbox coordinates)\n",
        "notes.json --> some info about this dataset (i.e. not used)\n",
        "```\n",
        "\n",
        "For training with YOLOv8 (from Ultralytics), you need to organize the files into `train` and `validate` (and optionally `test`) folders, and to create a `data.yaml` file to provide information about the folder location of test and validation set:\n",
        "\n",
        "```\n",
        "<root folder>\n",
        "--train\n",
        "----images\n",
        "----labels\n",
        "--valid\n",
        "----images\n",
        "----labels\n",
        "data.yaml\n",
        "```\n",
        "\n",
        "The data.yaml file should specify the following:\n",
        "```\n",
        "train: ../train/images\n",
        "val: ../valid/images\n",
        "test: ../test/images\n",
        "\n",
        "names:\n",
        "    0: balloon\n",
        "```\n",
        "\n",
        "If you have more than one class of object to detect, specify the rest of the names under the names field.\n"
      ],
      "metadata": {
        "id": "GgMJJTcNQ7us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Roboflow\n",
        "\n",
        "Alternatively, you can use the online service Roboflow to do annotation. Roboflow integrates very well with Ultralytics and you can easily export the dataset in a format recognized by Ultralytics trainer (for YOLO model)\n",
        "\n",
        "You can create a new account with [Roboflow](https://roboflow.com/).\n",
        "\n",
        "Similarly, you can create a new project, upload all the raw images, annotate them and then export.\n",
        "\n",
        "You can choose the format to be YOLOv8 and choose local directory to download the dataset locally instead of pushing it to the Roboflow universal wish.\n",
        "\n",
        "Here is a [introductory blog](https://blog.roboflow.com/getting-started-with-roboflow/) on using the Roboflow to annotate.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JxOlLlBJb0kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto Labelling using Grounding DINO\n",
        "\n",
        "Both Label Studio and Roboflow supports the use of Grounding DINO to auto label the dataset.\n",
        "\n",
        "[Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) is open-set object detector, marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs (prompts) such as category names or referring expressions.\n",
        "\n",
        "### Using Grounding DINO with Label Studio\n",
        "\n",
        "You can follow the instruction [here](https://labelstud.io/blog/using-text-prompts-for-image-annotation-with-grounding-dino-and-label-studio/)  to setup the Grounding DINO ML backend to integrate with your label studio.\n",
        "\n",
        "### Using Grounding DINO with Roboflow\n",
        "\n",
        "Here is a [video tutorial](https://youtu.be/SDV6Gz0suAk) on using Grounding DINO with Roboflow.\n"
      ],
      "metadata": {
        "id": "xPGO-z8rlqH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Annotated Dataset\n",
        "\n",
        "To save you time for this lab, you can download a pre-annotated balloon dataset [here](https://github.com/nyp-sit/iti107-2024S2/raw/refs/heads/main/data/balloon_annotated_dataset.zip).\n",
        "\n",
        "We download and unzip to the directory called `datasets`\n",
        "\n"
      ],
      "metadata": {
        "id": "v_VTuEfec2YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "wget https://github.com/nyp-sit/iti107-2024S2/raw/refs/heads/main/data/balloon_annotated_dataset.zip\n",
        "mkdir -p datasets\n",
        "unzip balloon_annotated_dataset.zip -d datasets/"
      ],
      "metadata": {
        "id": "yVh2OFXvtiM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BTQ6QpZz3kP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ultralytics\n",
        "!pip install comet_ml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "YOLOv8 comes with different sizes of pretrained models: yolov8n, yolov8s, .... They differs in terms of their sizes, inference speeds and mean average precision:\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/yolo-models.png?raw=true\" width=\"70%\"/>\n",
        "\n",
        "\n",
        "We will use the small pretrained model yolo8s and finetune it on our custom dataset.\n"
      ],
      "metadata": {
        "id": "YgyWGNT4-MLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup the logging\n",
        "\n",
        "Ultralytics support logging to `wandb`, `comet.ml` and `tensorboard`, out of the box. Here we only enable wandb.\n",
        "\n",
        "You need to create an account at [`wandb`](https://wandb.ai) and get the API key from https://wandb.ai/authorize.\n"
      ],
      "metadata": {
        "id": "pKnPWyDwUfvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import settings\n",
        "\n",
        "settings.update({\"wandb\": True,\n",
        "                 \"comet.ml\": False,\n",
        "                 \"tensorboard\": False})"
      ],
      "metadata": {
        "id": "FbMwi27fRsyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "We specify the path to `data.yaml` file, and train with a batch size of 15, and we also save the checkpoint at each epoch (save_period=1). We assume here you are connected to a GPU, hence we can specify the device to use as `device=0` to select the first GPU.  We specify the project name as `balloon`, this will create a folder called `balloon` to store the weights and various training artifacts such as F1, PR curves, confusion matrics, training results (loss, mAP, etc).\n",
        "\n",
        "For a complete listing of train settings, you can see [here](https://docs.ultralytics.com/modes/train/#train-settings).\n",
        "\n",
        "You can also specify the type of data [augmentation](https://docs.ultralytics.com/modes/train/#augmentation-settings-and-hyperparameters)  you want as part of the train pipeline.\n",
        "\n",
        "You can monitor your training progress at wandb (the link is given in the train output below)\n"
      ],
      "metadata": {
        "id": "iq9gV4A1VHlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la datasets/valid/images | wc -l"
      ],
      "metadata": {
        "id": "zrnbwsmcpYE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwjnKk02x-cF"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from ultralytics import settings\n",
        "\n",
        "model = YOLO(\"yolov8s.pt\")  # Load a pre-trained YOLO model\n",
        "result = model.train(data=\"datasets/data.yaml\",\n",
        "                     epochs=30,\n",
        "                     save_period=1,\n",
        "                     batch=16,\n",
        "                     device=0,\n",
        "                     project='balloon',\n",
        "                     plots=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the various graphs in your wandb dashboard, for example:\n",
        "\n",
        "*metrics*\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/wandb-metrics.png?raw=true\"/>\n",
        "\n",
        "*Train and validation loss*\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/wandb-loss.png?raw=true\"/>"
      ],
      "metadata": {
        "id": "59BONHriXz3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can go to the folder `balloon-->train-->weights` and you will files like epoch0.pt, epoch1.pt, .... and also best.pt.\n",
        "The epoch0.pt, epoch1.pt are the checkpoints that are saved every period (in our case, we specify period as 1 epoch).  The best.pt contains the best checkpoint."
      ],
      "metadata": {
        "id": "P3QivGwNaY3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run the best model (using the best checkpoint) against the validation dataset to see the overall model performance on validation set.  \n",
        "\n",
        "You should see around `0.88` for `mAP50`, and `0.78` for `mAP50-95`."
      ],
      "metadata": {
        "id": "G-PxfxzMbAOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n6GRS5f2f05"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"balloon/train/weights/best.pt\")\n",
        "validation_results = model.val(data=\"datasets/data.yaml\", device=\"0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export and Deployment\n",
        "\n",
        "Your model is in pytorch format (.pt). You can export the model to various format, e.g. TorchScript, ONNX, OpenVINO, TensorRT, etc. depending on your use case, and deployment platform (e.g. CPU or GPU, etc)\n",
        "\n",
        "You can see the list of [supported formats](https://docs.ultralytics.com/modes/export/#export-formats)  and the option they support in terms of further optimization (such as imagesize, int8, half-precision, etc) in the ultralytics site.\n",
        "\n",
        "Ultralytics provide a utility function to benchmark your model using different supported formats automatically. You can uncomment the code in the following code cell to see the benchmark result. If you are benchmarking for CPU only, the change the `device=0` to `device='cpu'`.  \n",
        "\n",
        "**Beware: it will take quite a while to complete the benchmark**"
      ],
      "metadata": {
        "id": "bio2cKcnb-Z-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usgkfg87bZBO"
      },
      "outputs": [],
      "source": [
        "# from ultralytics.utils.benchmarks import benchmark\n",
        "\n",
        "# # Benchmark on GPU (device=0 means the 1st GPU device)\n",
        "# benchmark(model=\"balloon/train/weights/best.pt\", data=\"datasets/data.yaml\", imgsz=640, half=False, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the following code, we export it as OpenVINO. OpenVINO is optimized for inference on Intel CPUs and since we will use the model later on to do inference on local Windows machine (which runs Intel chip), we will export it as OpenVINO format. We also specify using int8 quantization, which results in faster inference, at the cost of accuracy.\n",
        "\n",
        "For more information on OpenVINO, go to the [official documentation](https://docs.openvino.ai/2024/index.html).\n",
        "\n",
        "After export, you can find the openvino model in `balloon\\train\\weights\\best_openvino_model` directory."
      ],
      "metadata": {
        "id": "OD-vKnPYfBu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6PDNf5NaPch"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"balloon/train/weights/best.pt\")\n",
        "exported_path = model.export(format=\"openvino\", int8=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Let's test our model on some sample pictures. You can optionally specify the confidence threshold (e.g. `conf=0.5`), and the IoU (e.g. `iou=0.6`) for the NMS. The model will only output the bounding boxes of those detection that exceeds the confidence threshould and the IoU threshold.  "
      ],
      "metadata": {
        "id": "r11lPCYrfMKN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziKNFammhMxv"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "source = 'https://raw.githubusercontent.com/nyp-sit/iti107-2024S2/refs/heads/main/session-3/samples/sample_balloon.jpeg'\n",
        "#source = './samples/sample_balloon.jpeg'\n",
        "model = YOLO(\"balloon/train/weights/best_int8_openvino_model\", task='detect')\n",
        "result = model(source, conf=0.5, iou=0.6)\n",
        "\n",
        "# Visualize the results\n",
        "for i, r in enumerate(result):\n",
        "    print(r)\n",
        "    # Plot results image\n",
        "    im_bgr = r.plot()  # BGR-order numpy array\n",
        "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
        "\n",
        "    # Show results to screen (in supported environments)\n",
        "    r.show()\n",
        "\n",
        "    # Save results to disk\n",
        "    r.save(filename=f\"results{i}.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Model\n",
        "\n",
        "If you are training your model on Google Colab, you will download the exported OpenVINO model to a local PC. If you are training your model locally, then the exported model should already be on your local PC.\n",
        "\n",
        "Run the following code to zip up the OpenVINO folder and download to local PC."
      ],
      "metadata": {
        "id": "ygXaw7lxu8HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: If you encountered error message \"NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\", uncomment the following cell and run it.*\n"
      ],
      "metadata": {
        "id": "xTTV-ndvuOx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "_O-nmcYRuMIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mv ./balloon/train/weights/best_int8_openvino_model/ .\n",
        "zip -r best_int8_openvino_model.zip best_int8_openvino_model\n",
        "\n",
        "# Now go to best_openvino_model to download the best_openvino_model.zip file"
      ],
      "metadata": {
        "id": "j_CI0ZBTteA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui8i_pB5sItu"
      },
      "source": [
        "## Streaming\n",
        "\n",
        "We can also do real-time detection on a video or camera steram.\n",
        "\n",
        "The code below uses openCV library to display video in a window, and can only be run locally on a local laptop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video File\n",
        "\n",
        "You need `OpenCV` to run the following code.  In your conda environment, install `opencv` for python using the following command:\n",
        "\n",
        "```\n",
        "pip3 install opencv-python\n",
        "```\n",
        "or\n",
        "```\n",
        "conda install opencv\n",
        "```\n",
        "\n",
        "Let's donwload the sample video file."
      ],
      "metadata": {
        "id": "MmzL3StT5_CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/nyp-sit/iti107-2024S2/refs/heads/main/session-3/samples/balloon.mp4"
      ],
      "metadata": {
        "id": "DNK13LNd87G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming and display video"
      ],
      "metadata": {
        "id": "2nvf5BvM9_q2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TWG6JBVPE86"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO(\"best_int8_openvino_model\", task=\"detect\")\n",
        "\n",
        "# Open the video file\n",
        "video_path = \"balloon.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    # Read a frame from the video\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    if success:\n",
        "        # Run YOLO inference on the frame on GPU Device 0\n",
        "        results = model(frame, device=\"cpu\")\n",
        "\n",
        "        # Visualize the results on the frame\n",
        "        annotated_frame = results[0].plot()\n",
        "\n",
        "        # Display the annotated frame\n",
        "        cv2.imshow(\"YOLO Inference\", annotated_frame)\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detect and write to a video file"
      ],
      "metadata": {
        "id": "AZoRxssK98O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "# from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def write_video(video_in_filepath, video_out_filepath, model):\n",
        "    # Open the video file\n",
        "\n",
        "    video_reader = cv2.VideoCapture(video_in_filepath)\n",
        "\n",
        "    nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    fps = video_reader.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    video_writer = cv2.VideoWriter(video_out_filepath,\n",
        "                            cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                            fps,\n",
        "                            (frame_w, frame_h))\n",
        "\n",
        "    # Loop through the video frames\n",
        "    for i in tqdm(range(nb_frames)):\n",
        "        # Read a frame from the video\n",
        "        success, frame = video_reader.read()\n",
        "\n",
        "        if success:\n",
        "            # Run YOLO inference on the frame on GPU Device 0\n",
        "            results = model(frame, conf=0.9, device=0)\n",
        "\n",
        "            # Visualize the results on the frame\n",
        "            annotated_frame = results[0].plot()\n",
        "\n",
        "            # Write the annotated frame\n",
        "            video_writer.write(annotated_frame)\n",
        "\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    cv2.waitKey(1)\n"
      ],
      "metadata": {
        "id": "fqXYqdRC64mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "video_in_file = \"balloon.mp4\"\n",
        "basename = Path(video_in_file).stem\n",
        "video_out_file = os.path.join(basename + '_detected' + '.mp4')\n",
        "model = YOLO(\"best_int8_openvino_model\", task=\"detect\")\n",
        "write_video(video_in_file, video_out_file, model)"
      ],
      "metadata": {
        "id": "oT_bq-gC8q2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Sr-_DGxyRT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}